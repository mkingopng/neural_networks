{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Reinforcement Learning (DQN) Tutorial\n",
    "=====================================\n",
    "In this exercise, you will practice how to use PyTorch to train a Deep Q-learning (DQN) agent\n",
    "on the CartPole-v0 task from OpenAI Gym. Specifically, you will need to implement several functions/classes which are necessary components of the DQN algorithm:\n",
    "\n",
    "1.  ``ReplayMemory``\n",
    "2.  ``Q-Network``\n",
    "3.  ``Optimize_Model``\n",
    "4.  ``Select_action``\n",
    "\n",
    "Each function/class has its own cell where you can see more details, please complete the exercices marked with TODO.\n",
    "\n",
    "**Packages**\n",
    "\n",
    "We need OpenAI gym for the environment (Install using `pip install gym`)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "\n",
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperparameters\n",
    "After implementing the neural network model and other necessary functions, you can try to do more hyperparameters tuning."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "EPISODES = 400  # number of episodes\n",
    "EPS_START = 0.9  # e-greedy threshold start value\n",
    "EPS_END = 0.05  # e-greedy threshold end value\n",
    "EPS_DECAY = 200  # e-greedy threshold decay\n",
    "GAMMA = 0.99  # Q-learning discount factor\n",
    "LR = 0.01  # NN optimizer learning rate\n",
    "HIDDEN_LAYER = 64  # NN hidden layer size\n",
    "BATCH_SIZE = 64  # Q-learning batch size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Environment\n",
    "CartPole-v0 is a classic reinforcement learning environment from OpenAI Gym. In this environment, the agent has to decide between two actions $-$ moving the cart left or right $-$ so that the pole attached to it stays upright.\n",
    "\n",
    "As the agent observes the current state of the environment and chooses an action, the environment transitions to a new state, and also returns a reward that indicates the consequences of the action. In this task, rewards are +1 for every incremental timestep and the environment terminates if the pole falls over too far or the cart moves more then 2.4 units away from center. This means better performing scenarios will run for longer duration, accumulating larger return. The CartPole task is designed so that the inputs to the agent are 4 real values representing the environment state (position, velocity, etc.).\n",
    "\n",
    "We first set up the envrionment of CartPole-v0 using Gym."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env._max_episode_steps = 500\n",
    "env = wrappers.Monitor(env, './tmp/cartpole-v0-1', force=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Replay Memory\n",
    "-------------\n",
    "We will be using experience replay memory for training our DQN. It stores\n",
    "the transitions that the agent observes, allowing us to reuse this data\n",
    "later. By sampling from it randomly, the transitions that build up a\n",
    "batch are decorrelated. It has been shown that this greatly stabilizes\n",
    "and improves the DQN training procedure.\n",
    "\n",
    "For this, we're going to implement the replay memory buffer as a python class:\n",
    "\n",
    "-  ``ReplayMemory`` $-$ a cyclic buffer of bounded size that holds the\n",
    "   transitions observed recently. It also implements a ``.sample()``\n",
    "   method for selecting a random batch of transitions for training, and a ``.push()`` method for adding a new transition while potentially remove the oldest saved transition if the size of memory buffer exceeds the capacity. Each tranisiton is a tuple which consists of state, action, next_state, reward."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, transition):\n",
    "        self.memory.append(transition)\n",
    "        if len(self.memory) > self.capacity:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q-Network\n",
    "Next, we need to define our model. Our model will consist of fully connected layers that takes in the\n",
    "state returned by the envrionment. It has two\n",
    "outputs, representing $Q(s, \\mathrm{left})$ and\n",
    "$Q(s, \\mathrm{right})$ (where $s$ is the input to the\n",
    "network). In effect, the network is trying to predict the *expected return* of\n",
    "taking each action given the current input.\n",
    "\n",
    "Define a 2-layer fully connected neural network with ${\\rm tanh}$ activation at the hidden layer, followed by the output layer. The hidden layer size is decided by the hyperparameter 'HIDDEN_LAYER' and the size of the output is 2. You could also try any other architectures you want."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        nn.Module.__init__(self)\n",
    "        self.l1 = nn.Linear(4, HIDDEN_LAYER)\n",
    "        self.l2 = nn.Linear(HIDDEN_LAYER, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.l1(x))\n",
    "        x = self.l2(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create the model, memory buffer and optimizer."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "model = Network()\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "memory = ReplayMemory(10000)\n",
    "optimizer = optim.Adam(model.parameters(), LR)\n",
    "steps_done = 0\n",
    "episode_durations = []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "DQN algorithm\n",
    "-------------\n",
    "\n",
    "Our environment is deterministic, so all equations presented here are\n",
    "also formulated deterministically for the sake of simplicity. In the\n",
    "reinforcement learning literature, they would also contain expectations\n",
    "over stochastic transitions in the environment.\n",
    "\n",
    "Our aim will be to train a policy that tries to maximize the discounted,\n",
    "cumulative reward\n",
    "$R_{t_0} = \\sum_{t=t_0}^{\\infty} \\gamma^{t - t_0} r_t$, where\n",
    "$R_{t_0}$ is also known as the *return*. The discount,\n",
    "$\\gamma$, should be a constant between $0$ and $1$\n",
    "that ensures the sum converges. It makes rewards from the uncertain far\n",
    "future less important for our agent than the ones in the near future\n",
    "that it can be fairly confident about.\n",
    "\n",
    "The main idea behind Q-learning is that if we had a function\n",
    "$Q^*: State \\times Action \\rightarrow \\mathbb{R}$, that could tell\n",
    "us what our return would be, if we were to take an action in a given\n",
    "state, then we could easily construct a policy that maximizes our\n",
    "rewards:\n",
    "\n",
    "$$\\pi^*(s) = \\arg\\!\\max_a \\ Q^*(s, a)$$\n",
    "\n",
    "However, we don't know everything about the world, so we don't have\n",
    "access to $Q^*$. But, since neural networks are universal function\n",
    "approximators, we can simply create one and train it to resemble\n",
    "$Q^*$.\n",
    "\n",
    "For our training update rule, we'll use a fact that every $Q$\n",
    "function for some policy obeys the Bellman equation:\n",
    "\n",
    "$$Q^{\\pi}(s, a) = r + \\gamma\\,Q^{\\pi}(s', \\pi(s'))$$\n",
    "\n",
    "The difference between the two sides of the equality is known as the\n",
    "temporal difference error, $\\delta$:\n",
    "\n",
    "$$\\delta = Q(s, a) - (r + \\gamma \\max_b Q(s', b))$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training\n",
    "--------\n",
    "\n",
    "First, we need to implement some utility functions for our training procedure\n",
    "\n",
    "-  ``select_action`` $-$ will select an action accordingly to an epsilon\n",
    "   greedy policy. Simply put, we'll sometimes use our model for choosing\n",
    "   the action, and sometimes we'll just sample one uniformly. The\n",
    "   probability of choosing a random action will start at ``EPS_START``\n",
    "   and will decay exponentially towards ``EPS_END``. ``EPS_DECAY``\n",
    "   controls the rate of the decay.\n",
    "\n",
    "-  ``optimize_model`` $-$ performs a single step of the optimization. It first samples a batch, concatenates\n",
    "all the tensors into a single one, then we'll use the model to calculate the Q values for different state and use bellman euqation to optmize our model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        return model(Variable(state).type(FloatTensor)).data.max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return LongTensor([[random.randrange(2)]])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    # random transition batch is taken from experience replay memory\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch_state, batch_action, batch_next_state, batch_reward = zip(*transitions)\n",
    "    batch_state = Variable(torch.cat(batch_state))\n",
    "    batch_action = Variable(torch.cat(batch_action))\n",
    "    batch_reward = Variable(torch.cat(batch_reward))\n",
    "    batch_next_state = Variable(torch.cat(batch_next_state))\n",
    "\n",
    "    # current Q values are estimated by NN for all actions\n",
    "    current_q_values = model(batch_state).gather(1, batch_action)\n",
    "    # expected Q values are estimated from actions which gives maximum Q value\n",
    "    max_next_q_values = model(batch_next_state).detach().max(1)[0]\n",
    "    expected_q_values = batch_reward + (GAMMA * max_next_q_values)\n",
    "\n",
    "    # loss is measured from error between current and newly expected Q values\n",
    "    loss = F.smooth_l1_loss(current_q_values.squeeze(), expected_q_values)\n",
    "\n",
    "    # backpropagation of loss to NN\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below, you can find the main training loop. At the beginning we reset\n",
    "the environment and initialize the ``state`` Tensor. Then, we sample\n",
    "an action, execute it, observe the next state and the reward (always\n",
    "1), and optimize our model once. When the episode ends (our model\n",
    "fails), we restart the loop."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[99m Episode 0 finished after 28 steps\n",
      "\u001B[99m Episode 1 finished after 10 steps\n",
      "\u001B[99m Episode 2 finished after 25 steps\n",
      "\u001B[99m Episode 3 finished after 19 steps\n",
      "\u001B[99m Episode 4 finished after 10 steps\n",
      "\u001B[99m Episode 5 finished after 14 steps\n",
      "\u001B[99m Episode 6 finished after 18 steps\n",
      "\u001B[99m Episode 7 finished after 12 steps\n",
      "\u001B[99m Episode 8 finished after 9 steps\n",
      "\u001B[99m Episode 9 finished after 14 steps\n",
      "\u001B[99m Episode 10 finished after 9 steps\n",
      "\u001B[99m Episode 11 finished after 11 steps\n",
      "\u001B[99m Episode 12 finished after 9 steps\n",
      "\u001B[99m Episode 13 finished after 15 steps\n",
      "\u001B[99m Episode 14 finished after 9 steps\n",
      "\u001B[99m Episode 15 finished after 9 steps\n",
      "\u001B[99m Episode 16 finished after 9 steps\n",
      "\u001B[99m Episode 17 finished after 17 steps\n",
      "\u001B[99m Episode 18 finished after 8 steps\n",
      "\u001B[99m Episode 19 finished after 13 steps\n",
      "\u001B[99m Episode 20 finished after 11 steps\n",
      "\u001B[99m Episode 21 finished after 9 steps\n",
      "\u001B[99m Episode 22 finished after 10 steps\n",
      "\u001B[99m Episode 23 finished after 12 steps\n",
      "\u001B[99m Episode 24 finished after 10 steps\n",
      "\u001B[99m Episode 25 finished after 19 steps\n",
      "\u001B[99m Episode 26 finished after 22 steps\n",
      "\u001B[99m Episode 27 finished after 12 steps\n",
      "\u001B[99m Episode 28 finished after 10 steps\n",
      "\u001B[99m Episode 29 finished after 11 steps\n",
      "\u001B[99m Episode 30 finished after 10 steps\n",
      "\u001B[99m Episode 31 finished after 79 steps\n",
      "\u001B[99m Episode 32 finished after 19 steps\n",
      "\u001B[99m Episode 33 finished after 15 steps\n",
      "\u001B[99m Episode 34 finished after 28 steps\n",
      "\u001B[99m Episode 35 finished after 31 steps\n",
      "\u001B[99m Episode 36 finished after 48 steps\n",
      "\u001B[99m Episode 37 finished after 37 steps\n",
      "\u001B[99m Episode 38 finished after 29 steps\n",
      "\u001B[99m Episode 39 finished after 35 steps\n",
      "\u001B[99m Episode 40 finished after 39 steps\n",
      "\u001B[99m Episode 41 finished after 64 steps\n",
      "\u001B[99m Episode 42 finished after 20 steps\n",
      "\u001B[99m Episode 43 finished after 14 steps\n",
      "\u001B[99m Episode 44 finished after 27 steps\n",
      "\u001B[99m Episode 45 finished after 53 steps\n",
      "\u001B[99m Episode 46 finished after 43 steps\n",
      "\u001B[99m Episode 47 finished after 9 steps\n",
      "\u001B[99m Episode 48 finished after 10 steps\n",
      "\u001B[99m Episode 49 finished after 10 steps\n",
      "\u001B[99m Episode 50 finished after 9 steps\n",
      "\u001B[99m Episode 51 finished after 9 steps\n",
      "\u001B[99m Episode 52 finished after 9 steps\n",
      "\u001B[99m Episode 53 finished after 9 steps\n",
      "\u001B[99m Episode 54 finished after 12 steps\n",
      "\u001B[99m Episode 55 finished after 18 steps\n",
      "\u001B[99m Episode 56 finished after 10 steps\n",
      "\u001B[99m Episode 57 finished after 25 steps\n",
      "\u001B[99m Episode 58 finished after 69 steps\n",
      "\u001B[99m Episode 59 finished after 77 steps\n",
      "\u001B[99m Episode 60 finished after 51 steps\n",
      "\u001B[99m Episode 61 finished after 37 steps\n",
      "\u001B[99m Episode 62 finished after 10 steps\n",
      "\u001B[99m Episode 63 finished after 10 steps\n",
      "\u001B[99m Episode 64 finished after 8 steps\n",
      "\u001B[99m Episode 65 finished after 10 steps\n",
      "\u001B[99m Episode 66 finished after 9 steps\n",
      "\u001B[99m Episode 67 finished after 10 steps\n",
      "\u001B[99m Episode 68 finished after 8 steps\n",
      "\u001B[99m Episode 69 finished after 9 steps\n",
      "\u001B[99m Episode 70 finished after 9 steps\n",
      "\u001B[99m Episode 71 finished after 8 steps\n",
      "\u001B[99m Episode 72 finished after 10 steps\n",
      "\u001B[99m Episode 73 finished after 33 steps\n",
      "\u001B[99m Episode 74 finished after 66 steps\n",
      "\u001B[99m Episode 75 finished after 41 steps\n",
      "\u001B[99m Episode 76 finished after 50 steps\n",
      "\u001B[99m Episode 77 finished after 44 steps\n",
      "\u001B[99m Episode 78 finished after 123 steps\n",
      "\u001B[99m Episode 79 finished after 9 steps\n",
      "\u001B[99m Episode 80 finished after 9 steps\n",
      "\u001B[99m Episode 81 finished after 20 steps\n",
      "\u001B[99m Episode 82 finished after 84 steps\n",
      "\u001B[99m Episode 83 finished after 24 steps\n",
      "\u001B[99m Episode 84 finished after 89 steps\n",
      "\u001B[99m Episode 85 finished after 21 steps\n",
      "\u001B[99m Episode 86 finished after 15 steps\n",
      "\u001B[99m Episode 87 finished after 12 steps\n",
      "\u001B[99m Episode 88 finished after 10 steps\n",
      "\u001B[99m Episode 89 finished after 8 steps\n",
      "\u001B[99m Episode 90 finished after 18 steps\n",
      "\u001B[99m Episode 91 finished after 16 steps\n",
      "\u001B[99m Episode 92 finished after 18 steps\n",
      "\u001B[99m Episode 93 finished after 18 steps\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [20]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      3\u001B[0m steps \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m----> 5\u001B[0m     \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      6\u001B[0m     action \u001B[38;5;241m=\u001B[39m select_action(FloatTensor([state]))\n\u001B[1;32m      7\u001B[0m     next_state, reward, done, _ \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(action[\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mitem())\n",
      "File \u001B[0;32m~/anaconda3/envs/standardenvironment/lib/python3.9/site-packages/gym/core.py:240\u001B[0m, in \u001B[0;36mWrapper.render\u001B[0;34m(self, mode, **kwargs)\u001B[0m\n\u001B[1;32m    239\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrender\u001B[39m(\u001B[38;5;28mself\u001B[39m, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhuman\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 240\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/standardenvironment/lib/python3.9/site-packages/gym/core.py:240\u001B[0m, in \u001B[0;36mWrapper.render\u001B[0;34m(self, mode, **kwargs)\u001B[0m\n\u001B[1;32m    239\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrender\u001B[39m(\u001B[38;5;28mself\u001B[39m, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhuman\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 240\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/standardenvironment/lib/python3.9/site-packages/gym/envs/classic_control/cartpole.py:213\u001B[0m, in \u001B[0;36mCartPoleEnv.render\u001B[0;34m(self, mode)\u001B[0m\n\u001B[1;32m    210\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcarttrans\u001B[38;5;241m.\u001B[39mset_translation(cartx, carty)\n\u001B[1;32m    211\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpoletrans\u001B[38;5;241m.\u001B[39mset_rotation(\u001B[38;5;241m-\u001B[39mx[\u001B[38;5;241m2\u001B[39m])\n\u001B[0;32m--> 213\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mviewer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreturn_rgb_array\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrgb_array\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/standardenvironment/lib/python3.9/site-packages/gym/envs/classic_control/rendering.py:127\u001B[0m, in \u001B[0;36mViewer.render\u001B[0;34m(self, return_rgb_array)\u001B[0m\n\u001B[1;32m    125\u001B[0m     arr \u001B[38;5;241m=\u001B[39m arr\u001B[38;5;241m.\u001B[39mreshape(buffer\u001B[38;5;241m.\u001B[39mheight, buffer\u001B[38;5;241m.\u001B[39mwidth, \u001B[38;5;241m4\u001B[39m)\n\u001B[1;32m    126\u001B[0m     arr \u001B[38;5;241m=\u001B[39m arr[::\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m,:,\u001B[38;5;241m0\u001B[39m:\u001B[38;5;241m3\u001B[39m]\n\u001B[0;32m--> 127\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwindow\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflip\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    128\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39monetime_geoms \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    129\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m arr \u001B[38;5;28;01mif\u001B[39;00m return_rgb_array \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39misopen\n",
      "File \u001B[0;32m~/anaconda3/envs/standardenvironment/lib/python3.9/site-packages/pyglet/window/xlib/__init__.py:489\u001B[0m, in \u001B[0;36mXlibWindow.flip\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    487\u001B[0m \u001B[38;5;66;03m# TODO canvas.flip?\u001B[39;00m\n\u001B[1;32m    488\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontext:\n\u001B[0;32m--> 489\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflip\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    491\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sync_resize()\n",
      "File \u001B[0;32m~/anaconda3/envs/standardenvironment/lib/python3.9/site-packages/pyglet/gl/xlib.py:376\u001B[0m, in \u001B[0;36mXlibContext13.flip\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    374\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_vsync:\n\u001B[1;32m    375\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wait_vsync()\n\u001B[0;32m--> 376\u001B[0m \u001B[43mglx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mglXSwapBuffers\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mx_display\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mglx_window\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for e in range(EPISODES):\n",
    "    state = env.reset()\n",
    "    steps = 0\n",
    "    while True:\n",
    "        env.render()\n",
    "        action = select_action(FloatTensor([state]))\n",
    "        next_state, reward, done, _ = env.step(action[0, 0].item())\n",
    "        # negative reward when attempt ends\n",
    "        if done:\n",
    "            reward = -1\n",
    "        memory.push((FloatTensor([state]),\n",
    "                     action,  # action is already a tensor\n",
    "                     FloatTensor([next_state]),\n",
    "                     FloatTensor([reward])))\n",
    "\n",
    "        optimize_model()\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "\n",
    "        if done:\n",
    "            print(\"{2} Episode {0} finished after {1} steps\"\n",
    "                  .format(e, steps, '\\033[92m' if steps >= 195 else '\\033[99m'))\n",
    "            episode_durations.append(steps)\n",
    "            break\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
