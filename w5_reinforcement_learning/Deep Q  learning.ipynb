{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Reinforcement Learning (DQN) Tutorial\n",
    "=====================================\n",
    "In this exercise, you will practice how to use PyTorch to train a Deep Q-learning (DQN) agent\n",
    "on the CartPole-v0 task from OpenAI Gym. Specifically, you will need to implement several functions/classes which are necessary components of the DQN algorithm:\n",
    "\n",
    "1.  ``ReplayMemory``\n",
    "2.  ``Q-Network``\n",
    "3.  ``Optimize_Model``\n",
    "4.  ``Select_action``\n",
    "\n",
    "Each function/class has its own cell where you can see more details, please complete the exercices marked with TODO.\n",
    "\n",
    "**Packages**\n",
    "\n",
    "We need OpenAI gym for the environment (Install using `pip install gym`)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "\n",
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperparameters\n",
    "After implementing the neural network model and other necessary functions, you can try to do more hyperparameters tuning."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "EPISODES = 400  # number of episodes\n",
    "EPS_START = 0.9  # e-greedy threshold start value\n",
    "EPS_END = 0.05  # e-greedy threshold end value\n",
    "EPS_DECAY = 200  # e-greedy threshold decay\n",
    "GAMMA = 0.99  # Q-learning discount factor\n",
    "LR = 0.01  # NN optimizer learning rate\n",
    "HIDDEN_LAYER = 64  # NN hidden layer size\n",
    "BATCH_SIZE = 64  # Q-learning batch size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Environment\n",
    "CartPole-v0 is a classic reinforcement learning environment from OpenAI Gym. In this environment, the agent has to decide between two actions $-$ moving the cart left or right $-$ so that the pole attached to it stays upright.\n",
    "\n",
    "As the agent observes the current state of the environment and chooses an action, the environment transitions to a new state, and also returns a reward that indicates the consequences of the action. In this task, rewards are +1 for every incremental timestep and the environment terminates if the pole falls over too far or the cart moves more then 2.4 units away from center. This means better performing scenarios will run for longer duration, accumulating larger return. The CartPole task is designed so that the inputs to the agent are 4 real values representing the environment state (position, velocity, etc.).\n",
    "\n",
    "We first set up the envrionment of CartPole-v0 using Gym."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env._max_episode_steps = 500\n",
    "env = wrappers.Monitor(env, './tmp/cartpole-v0-1', force=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Replay Memory\n",
    "-------------\n",
    "We will be using experience replay memory for training our DQN. It stores\n",
    "the transitions that the agent observes, allowing us to reuse this data\n",
    "later. By sampling from it randomly, the transitions that build up a\n",
    "batch are decorrelated. It has been shown that this greatly stabilizes\n",
    "and improves the DQN training procedure.\n",
    "\n",
    "For this, we're going to implement the replay memory buffer as a python class:\n",
    "\n",
    "-  ``ReplayMemory`` $-$ a cyclic buffer of bounded size that holds the\n",
    "   transitions observed recently. It also implements a ``.sample()``\n",
    "   method for selecting a random batch of transitions for training, and a ``.push()`` method for adding a new transition while potentially remove the oldest saved transition if the size of memory buffer exceeds the capacity. Each tranisiton is a tuple which consists of state, action, next_state, reward."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, transition):\n",
    "        self.memory.append(transition)\n",
    "        if len(self.memory) > self.capacity:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q-Network\n",
    "Next, we need to define our model. Our model will consist of fully connected layers that takes in the\n",
    "state returned by the envrionment. It has two\n",
    "outputs, representing $Q(s, \\mathrm{left})$ and\n",
    "$Q(s, \\mathrm{right})$ (where $s$ is the input to the\n",
    "network). In effect, the network is trying to predict the *expected return* of\n",
    "taking each action given the current input.\n",
    "\n",
    "Define a 2-layer fully connected neural network with ${\\rm tanh}$ activation at the hidden layer, followed by the output layer. The hidden layer size is decided by the hyperparameter 'HIDDEN_LAYER' and the size of the output is 2. You could also try any other architectures you want."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        nn.Module.__init__(self)\n",
    "        self.l1 = nn.Linear(4, HIDDEN_LAYER)\n",
    "        self.l2 = nn.Linear(HIDDEN_LAYER, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.l1(x))\n",
    "        x = self.l2(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create the model, memory buffer and optimizer."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "model = Network()\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "memory = ReplayMemory(10000)\n",
    "optimizer = optim.Adam(model.parameters(), LR)\n",
    "steps_done = 0\n",
    "episode_durations = []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "DQN algorithm\n",
    "-------------\n",
    "\n",
    "Our environment is deterministic, so all equations presented here are\n",
    "also formulated deterministically for the sake of simplicity. In the\n",
    "reinforcement learning literature, they would also contain expectations\n",
    "over stochastic transitions in the environment.\n",
    "\n",
    "Our aim will be to train a policy that tries to maximize the discounted,\n",
    "cumulative reward\n",
    "$R_{t_0} = \\sum_{t=t_0}^{\\infty} \\gamma^{t - t_0} r_t$, where\n",
    "$R_{t_0}$ is also known as the *return*. The discount,\n",
    "$\\gamma$, should be a constant between $0$ and $1$\n",
    "that ensures the sum converges. It makes rewards from the uncertain far\n",
    "future less important for our agent than the ones in the near future\n",
    "that it can be fairly confident about.\n",
    "\n",
    "The main idea behind Q-learning is that if we had a function\n",
    "$Q^*: State \\times Action \\rightarrow \\mathbb{R}$, that could tell\n",
    "us what our return would be, if we were to take an action in a given\n",
    "state, then we could easily construct a policy that maximizes our\n",
    "rewards:\n",
    "\n",
    "$$\\pi^*(s) = \\arg\\!\\max_a \\ Q^*(s, a)$$\n",
    "\n",
    "However, we don't know everything about the world, so we don't have\n",
    "access to $Q^*$. But, since neural networks are universal function\n",
    "approximators, we can simply create one and train it to resemble\n",
    "$Q^*$.\n",
    "\n",
    "For our training update rule, we'll use a fact that every $Q$\n",
    "function for some policy obeys the Bellman equation:\n",
    "\n",
    "$$Q^{\\pi}(s, a) = r + \\gamma\\,Q^{\\pi}(s', \\pi(s'))$$\n",
    "\n",
    "The difference between the two sides of the equality is known as the\n",
    "temporal difference error, $\\delta$:\n",
    "\n",
    "$$\\delta = Q(s, a) - (r + \\gamma \\max_b Q(s', b))$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training\n",
    "--------\n",
    "\n",
    "First, we need to implement some utility functions for our training procedure\n",
    "\n",
    "-  ``select_action`` $-$ will select an action accordingly to an epsilon\n",
    "   greedy policy. Simply put, we'll sometimes use our model for choosing\n",
    "   the action, and sometimes we'll just sample one uniformly. The\n",
    "   probability of choosing a random action will start at ``EPS_START``\n",
    "   and will decay exponentially towards ``EPS_END``. ``EPS_DECAY``\n",
    "   controls the rate of the decay.\n",
    "\n",
    "-  ``optimize_model`` $-$ performs a single step of the optimization. It first samples a batch, concatenates\n",
    "all the tensors into a single one, then we'll use the model to calculate the Q values for different state and use bellman euqation to optmize our model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        return model(Variable(state).type(FloatTensor)).data.max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return LongTensor([[random.randrange(2)]])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    # random transition batch is taken from experience replay memory\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch_state, batch_action, batch_next_state, batch_reward = zip(*transitions)\n",
    "    batch_state = Variable(torch.cat(batch_state))\n",
    "    batch_action = Variable(torch.cat(batch_action))\n",
    "    batch_reward = Variable(torch.cat(batch_reward))\n",
    "    batch_next_state = Variable(torch.cat(batch_next_state))\n",
    "\n",
    "    # current Q values are estimated by NN for all actions\n",
    "    current_q_values = model(batch_state).gather(1, batch_action)\n",
    "    # expected Q values are estimated from actions which gives maximum Q value\n",
    "    max_next_q_values = model(batch_next_state).detach().max(1)[0]\n",
    "    expected_q_values = batch_reward + (GAMMA * max_next_q_values)\n",
    "\n",
    "    # loss is measured from error between current and newly expected Q values\n",
    "    loss = F.smooth_l1_loss(current_q_values.squeeze(), expected_q_values)\n",
    "\n",
    "    # backpropagation of loss to NN\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below, you can find the main training loop. At the beginning we reset\n",
    "the environment and initialize the ``state`` Tensor. Then, we sample\n",
    "an action, execute it, observe the next state and the reward (always\n",
    "1), and optimize our model once. When the episode ends (our model\n",
    "fails), we restart the loop."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_159454/2081075630.py:6: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484806139/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  action = select_action(FloatTensor([state]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[99m Episode 0 finished after 11 steps\n",
      "\u001B[99m Episode 1 finished after 16 steps\n",
      "\u001B[99m Episode 2 finished after 28 steps\n",
      "\u001B[99m Episode 3 finished after 12 steps\n",
      "\u001B[99m Episode 4 finished after 19 steps\n",
      "\u001B[99m Episode 5 finished after 15 steps\n",
      "\u001B[99m Episode 6 finished after 26 steps\n",
      "\u001B[99m Episode 7 finished after 13 steps\n",
      "\u001B[99m Episode 8 finished after 11 steps\n",
      "\u001B[99m Episode 9 finished after 9 steps\n",
      "\u001B[99m Episode 10 finished after 11 steps\n",
      "\u001B[99m Episode 11 finished after 10 steps\n",
      "\u001B[99m Episode 12 finished after 8 steps\n",
      "\u001B[99m Episode 13 finished after 9 steps\n",
      "\u001B[99m Episode 14 finished after 11 steps\n",
      "\u001B[99m Episode 15 finished after 13 steps\n",
      "\u001B[99m Episode 16 finished after 9 steps\n",
      "\u001B[99m Episode 17 finished after 14 steps\n",
      "\u001B[99m Episode 18 finished after 10 steps\n",
      "\u001B[99m Episode 19 finished after 13 steps\n",
      "\u001B[99m Episode 20 finished after 12 steps\n",
      "\u001B[99m Episode 21 finished after 12 steps\n",
      "\u001B[99m Episode 22 finished after 9 steps\n",
      "\u001B[99m Episode 23 finished after 9 steps\n",
      "\u001B[99m Episode 24 finished after 12 steps\n",
      "\u001B[99m Episode 25 finished after 11 steps\n",
      "\u001B[99m Episode 26 finished after 12 steps\n",
      "\u001B[99m Episode 27 finished after 11 steps\n",
      "\u001B[99m Episode 28 finished after 9 steps\n",
      "\u001B[99m Episode 29 finished after 12 steps\n",
      "\u001B[99m Episode 30 finished after 13 steps\n",
      "\u001B[99m Episode 31 finished after 9 steps\n",
      "\u001B[99m Episode 32 finished after 9 steps\n",
      "\u001B[99m Episode 33 finished after 10 steps\n",
      "\u001B[99m Episode 34 finished after 8 steps\n",
      "\u001B[99m Episode 35 finished after 58 steps\n",
      "\u001B[99m Episode 36 finished after 27 steps\n",
      "\u001B[99m Episode 37 finished after 32 steps\n",
      "\u001B[99m Episode 38 finished after 25 steps\n",
      "\u001B[99m Episode 39 finished after 28 steps\n",
      "\u001B[99m Episode 40 finished after 26 steps\n",
      "\u001B[99m Episode 41 finished after 20 steps\n",
      "\u001B[99m Episode 42 finished after 52 steps\n",
      "\u001B[99m Episode 43 finished after 26 steps\n",
      "\u001B[99m Episode 44 finished after 43 steps\n",
      "\u001B[99m Episode 45 finished after 41 steps\n",
      "\u001B[99m Episode 46 finished after 81 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for e in range(EPISODES):\n",
    "    state = env.reset()\n",
    "    steps = 0\n",
    "    while True:\n",
    "        env.render()\n",
    "        action = select_action(FloatTensor([state]))\n",
    "        next_state, reward, done, _ = env.step(action[0, 0].item())\n",
    "        # negative reward when attempt ends\n",
    "        if done:\n",
    "            reward = -1\n",
    "        memory.push((FloatTensor([state]),\n",
    "                     action,  # action is already a tensor\n",
    "                     FloatTensor([next_state]),\n",
    "                     FloatTensor([reward])))\n",
    "\n",
    "        optimize_model()\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "\n",
    "        if done:\n",
    "            print(\"{2} Episode {0} finished after {1} steps\"\n",
    "                  .format(e, steps, '\\033[92m' if steps >= 195 else '\\033[99m'))\n",
    "            episode_durations.append(steps)\n",
    "            break\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
