{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Consider an environment with two states $S = \\{\\ S_1, S_2 \\}\\$ and two actions $A = \\{\\ a_1, a_2 \\}\\$ where the (deterministic) transitions $\\delta$ and reward $R$ for each state and action are as follows:\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question 1:\n",
    "assuming a discount factor of \\gamma = 0.7$ determine the optimal policy $\\pi^*: S \\to A$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Answer\n",
    "$$\\pi^* (S_1) = a_2$$\n",
    "\n",
    "\n",
    "$$\\pi^* (S_2) = a_1$$\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question 2\n",
    "still assuming $\\gamma = 0.7$ determine the value function $V: S \\to R$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Answer\n",
    "$$V(S_1) = 5.69$$\n",
    "\n",
    "$$V(S_2) = 10.98$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question 3\n",
    "still assuming $\\gamma = 0.7$ determine the values of the Q-function $Q: S \\times A \\to R$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Answer\n",
    "\n",
    "\\begin{aligned}\n",
    "Q(S_1, a_1) &= 1 + \\gamma V (S_1) &= 4.98 \\\\\n",
    "Q(S_1, a_2) &= V(S_1) &= 5.69 \\\\\n",
    "Q(S_2, a_1) &= V(S_2) &= 10.98 \\\\\n",
    "Q(S_2, a_2) &= 3 + \\gamma V(S_2) &= 10.69 \\\\\n",
    "\\end{aligned}\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question 4\n",
    "Still assuming $\\gamma = 0.7$, trace through the first few steps of the Q-learning algorithm, assuming a learning rate of 1 and with all Q values initially set to zero. Explain why it is necessary to force exploration through probabilistic choice of actions, in order to ensure convergence to the true Q values.\n",
    "\n",
    "Here are some hints to get you started:\n",
    "\n",
    "Since the learning rate is 1 (and the environment deterministic) we can use this Q-Learning update rule:\n",
    "\n",
    "$$Q(S, a) \\leftarrow r(S,a) + \\gamma \\max Q(\\delta(S, a), b)$$\n",
    "\n",
    "Let's assume the agent starts in state . Because the initial Q values are all zero, the first action must be chosen randomly. If action is chosen, the agent will get a reward of +1 and the update will be\n",
    "\n",
    "$$Q(S_1, a_1) \\leftarrow + \\gamma \\times 0 = 1$$\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Answer:\n",
    "With a deterministic environment and a learning rate of 1, the Q-learning update rule is\n",
    "\n",
    "$Q(S,a) \\leftarrow r(S,a) + \\gamma \\max Q(\\delta(S,a),b)$\n",
    "\n",
    "if the agent starts in a state $S_1$ and chooses action $a_1$, it will get a reward of +1 and the update will be:\n",
    "\n",
    "$$Q(S_1, a_1) \\leftarrow 1 + \\gamma \\times 0 = 1$$\n",
    "\n",
    "we do $\\textbf{not}$ force exploration, the agent will always prefer action $a_1$ in state $S_1$, and will never explore action $a_2$ this means that $Q(S_1, a_2)$ will remain zero forever, instead of converging to the true value of 5.69. If we $\\textbf{do}$ force exploration, the next steps may look like this:\n",
    "\n",
    "\\begin{center}\n",
    "\\begin{tabular}{ c c c }\n",
    " \\text{current state} & \\text{chosen state} & \\text{new Q value} \\\\\n",
    " $S_1$ & $a_2$ & $-2 + \\gamma \\times = -2$ \\\\\n",
    " $S_2$ & $a_2$ & $3 + \\gamma \\times 0 = 3$\n",
    "\\end{tabular}\n",
    "\\end{center}\n",
    "\n",
    "At this point the table looks like this:\n",
    "\n",
    "\\begin{center}\n",
    "\\begin{tabular}{ c c c }\n",
    " Q & $a_1$ & $a_2$ \\\\\n",
    " $S_1$ & 1 & -2 \\\\\n",
    " $S_2$ & 0 & 3\n",
    "\\end{tabular}\n",
    "\\end{center}\n",
    "\n",
    "Again we need to force exploration in order to the get the agent to choose $a_1$\n",
    "from $S_2$ and to again choose $a_2$ from $S_1$\n",
    "\n",
    "\\begin{center}\n",
    "\\begin{tabular}{ c c c }\n",
    " \\text{current state} & \\text{chosen action} & \\text{new Q value} \\\\\n",
    " $S_2$ & $a_1$ & $7 + \\gamma \\times 1 = 7.7$ \\\\\n",
    " $S_1$ & $a_2$ & $-2 + \\gamma 7.7 = 3.39$\n",
    "\\end{tabular}\n",
    "\\end{center}\n",
    "\n",
    "\\begin{center}\n",
    "\\begin{tabular}{ c c c }\n",
    " Q & $a_1$ & $a_2$ \\\\\n",
    " $S_1$ & 1 & 3.39 \\\\\n",
    " $S_2$ & 7.7 & 3\n",
    "\\end{tabular}\n",
    "\\end{center}\n",
    "\n",
    "Further steps will refine the Q value estimates, and in the limit they will\n",
    "converge to their true values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question 5\n",
    "Now let's consider how the value function changes as the discount factor  varies between 0 and 1.\n",
    "There are four deterministic policies for this environment, which can be written as\n",
    "\n",
    "Calculate the value function  for each of these four policies (keeping as a variable)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Answer:\n",
    "<img src=\"../out/images/5a_5.png\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question 6\n",
    "Determine for which range of values of  each of the policies  is optimal."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Answer:\n",
    "<img src=\"../out/images/5a_6.png\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../out/images/5a_6b.png\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question 1\n",
    "Describe the elements (sets and functions) that are needed to give a formal description of a reinforcement learning environment. What is the difference between a deterministic environment and a stochastic environment?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../out/images/revision 7 q1.png\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question 2\n",
    "Name three different models of optimality in reinforcement learning, and give a formula for calculating each one."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../out/images/revision 7 q2.png\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question 3\n",
    "What is the definition of:\n",
    "- the optimal policy\n",
    "- the value function\n",
    "- the Q-function?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../out/images/revision 7 q3.png\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question 4\n",
    "Assuming a stochastic environment, discount factor $\\gamma$ and learning rate of $\\eta$, write the equation for\n",
    "- Temporal Difference learning TD(0)\n",
    "- Q-Learning\n",
    "\n",
    "Remember to define any symbols you use."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../out/images/revision 7 q4.png\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
