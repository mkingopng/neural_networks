{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d7fed0f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### derivation of least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0d69ca",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9de1e492",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Softmax Exercise\n",
    "\n",
    "recall that the formula for Softmax is:\n",
    "\n",
    "$$\\text{Prob}(i) = \\dfrac{\\exp(z_i)}{\\sum_{N}^{j=1} \\exp(z_j)}$$\n",
    "\n",
    "$$\\log\\text{Prob}(i) = z_i - \\log \\sum_{j=1}^{N} \\exp(z_j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Consider a classification task with three classes 111, 222, 333. Suppose a particular input is presented, producing outputs:\n",
    "\n",
    "$$z_1 = 1$$\n",
    "$$z_2 = 2$$\n",
    "$$z_3 = 3$$\n",
    "\n",
    "and that the correct class is 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Question 1\n",
    "Compute each of the following to 2dp:\n",
    "\n",
    "Prob(1)\n",
    "Prob(2)\n",
    "Prob(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7f57e5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429ced08",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = T.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465f5cf2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "t1 = T.tensor([1.0, 2.0, 3.0], dtype=T.float32).to(device)\n",
    "sm = T.nn.functional.softmax(t1, dim=0)\n",
    "# lsm = T.nn.functional.log_softmax(t1, dim=0)\n",
    "# l_sm = T.log(T.nn.functional.softmax(t1, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37442876",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "T.set_printoptions(precision=4)\n",
    "print(\"tensor t1        = \", end=\"\"); print(t1)\n",
    "print(\"softmax(t1)      = \", end=\"\"); print(sm)\n",
    "# print(\"log_softmax(t1)  = \", end=\"\"); print(lsm)\n",
    "# print(\"log(softmax(t1)) = \", end=\"\"); print(l_sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5c85f1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### question 2\n",
    "Compute each of the following, to two decimal places:\n",
    "\n",
    "d(log Prob(2))/$dz_1$\n",
    "\n",
    "d(log Prob(2))/$dz_2$\n",
    "\n",
    "d(log Prob(2))/$dz_3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b157b49",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sympy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "z = 2.718"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "-0.09004527836777446"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expr = -z / (z + z**2 + z**3)\n",
    "expr  # d(log Prob(2))/$dz_1$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "0.755256933396389"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expr = 1 - (z**2 / (z + z**2 + z**3))\n",
    "expr  # d(log Prob(2))/$dz_2$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "-0.6652116550286146"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expr = -z**3 / (z + z**2 + z**3)\n",
    "expr  # d(log Prob(2))/$dz_2$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "481f86fb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Question 3\n",
    "Consider a degenerate case of supervised learning where the training set consists of just a single\n",
    "input, repeated 100 times. \n",
    "\n",
    "In 80 of the 100 cases, the target output value is 1; in the other 20, it is 0.\n",
    "\n",
    "What will a back-propagation neural network predict for this example, assuming that it has been\n",
    "trained and reaches a global minimum? Does it make a difierence whether the loss function is sum\n",
    "squared error or cross entropy? \n",
    "\n",
    "(**Hint**: to find the global minimum, differentiate the loss function and\n",
    "set the derivative to zero.)\n",
    "\n",
    "calculate the SSE and Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46616951",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2-3a: Sum of Squared Errors\n",
    "$E = \\dfrac{1}{2} \\sum_{i}(t_i - z_i)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb4be8c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sympy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291fc22d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "z = symbols('z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b187ca9e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "expr = -80 * log(z) - 20 * log(1-z)\n",
    "expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452378f7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "expr_1 = diff(expr, z)\n",
    "expr_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325aa36f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "solve(expr_1)  # this is equivalent to 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43a4f85",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2-3b: Cross Entropy\n",
    "$E = \\sum_{i} (-t_i\\log(z_i) - (1-t_i)\\log(1-z_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a993dcc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "expr_2 = diff(expr, z)\n",
    "expr_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c9cc0e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "solve(expr_2)  # this is equivalent to 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Question 1\n",
    "Explain the difference between the following paradigms, in terms of what is presaented to the system, and what it aims to achieve:\n",
    "\n",
    "- Supervised learning\n",
    "- Reinforcement Learning\n",
    "- Unsupervised Learning"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Answer:\n",
    "**Supervised Learning**: The system is presented with training items consisting of an input and a target output. The aim is to predict the output given the input (for the training set as well as an unseen test set)\n",
    "\n",
    "**Reinforcement Learning**: the system chooses actions in a simulated environment, observing its state and receiving reqards along the way. The aim is to maximise the cumulativ reward.\n",
    "\n",
    "**unsupervised learning**: the system is presented with training items consisting of only an input (no target value). The aim is to extract hidden features or other structure from these data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Question 2\n",
    "Explain what is meant by Overfitting in neural networks, and list four different methods for avoiding it."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Answer\n",
    "Overfitting is where the training set error continues to reduce but the test set error stalls or increases. This can be avoided by:\n",
    "- limiting the number of neurons or connection in the network\n",
    "- early stopping, with a validation set\n",
    "- dropout\n",
    "- weight decay (this can avoid over fitting by limiting the size of the weights)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Question 3\n",
    "Explain how Dropout is used for neural networks, in both the training and testing phase."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Answer\n",
    "During each minibatch of training, a fixed percentage (usually one half) of nodes are chosen to be inactive. In the testing phase, all nodes are active but the activation of each node is multiplied by the same percentage that was used during training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Question 4\n",
    "Write the formulas for these Loss functions: Squared Error, Cross Entropy, Softmax, Weight Decay (remember to define any variables you use)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Answer:\n",
    "assume $z_i$ is the actual output, $t_i$ is the target output and $w_j$ are the weights\n",
    "\n",
    "Squared error: $E = \\dfrac{1}{2} \\sum_i (z_i - t_i)^2$\n",
    "\n",
    "Cross Entropy: $E = \\sum_i (-t_i \\log z_i - (1 - t_i) \\log(1-z_i)$\n",
    "\n",
    "Softmax: $-( z_i - \\log \\sum_{j=1}^{N} \\exp(z_j))$\n",
    "\n",
    "Weight Decay: $E = \\dfrac{1}{2} \\sum_j w_j^2$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Question 5\n",
    "in the context of supervised learning, explain the difference between MLE and Bayesian inference."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Answer\n",
    "In MLE, the hypothesis $h \\in H$ is chosen which maximises the conditional probability $P(D|h)$ of the observed data $D$ conditioned on $H$\n",
    "\n",
    "In bayesian inference, the hypothesis $h \\in H$ is chosen which maximises $P(D|h)P(h)$ where $P(h)$ is the prior probability of $h$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Question 6\n",
    "Briefly explain the concept of Momentum, as an enhancement for Gradient Descent."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### answer\n",
    "\n",
    "a running average of the differentials for each weight is maintained and used to update the weights as follows:\n",
    "\n",
    "$$\\delta w = \\alpha \\delta w - \\eta \\dfrac{dE}{dw}$$\n",
    "\n",
    "$$w = w + \\delta w$$\n",
    "\n",
    "the constant $\\alpha$ with $0 \\leq \\alpha < 1$ is called momentum"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Derivation of Least Squares"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "1f9148fc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### compute softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540bd711",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42afbe90",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### compute weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2becb111",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22813c0c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### compute momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a1c29b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87d24a53",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### code simple pytorch operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67762e3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8536904",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### analyse the geometry of hidden unit activations in neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ceae0e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}